<!DOCTYPE html>
<html>
<head>
  <title>BL6024 - Quantitative Skills for Biologists using R</title>
  <meta charset="utf-8">
  <meta name="description" content="BL6024 - Quantitative Skills for Biologists using R">
  <meta name="author" content="">
  <meta name="generator" content="slidify" />
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta http-equiv="X-UA-Compatible" content="chrome=1">
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/default.css" media="all" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/phone.css" 
    media="only screen and (max-device-width: 480px)" >
  <link rel="stylesheet" href="libraries/frameworks/io2012/css/slidify.css" >
  <link rel="stylesheet" href="libraries/highlighters/highlight.js/css/tomorrow.css" />
  <base target="_blank"> <!-- This amazingness opens all links in a new tab. -->  <link rel=stylesheet href="./assets/css/ribbons.css"></link>
<link rel=stylesheet href="./assets/css/style.css"></link>

  
  <!-- Grab CDN jQuery, fall back to local if offline -->
  <script src="http://ajax.aspnetcdn.com/ajax/jQuery/jquery-1.7.min.js"></script>
  <script>window.jQuery || document.write('<script src="libraries/widgets/quiz/js/jquery.js"><\/script>')</script> 
  <script data-main="libraries/frameworks/io2012/js/slides" 
    src="libraries/frameworks/io2012/js/require-1.0.8.min.js">
  </script>
  
  

</head>
<body style="opacity: 0">
  <slides class="layout-widescreen">
    
    <!-- LOGO SLIDE -->
        <slide class="title-slide segue nobackground">
  <hgroup class="auto-fadein">
    <h1>BL6024 - Quantitative Skills for Biologists using R</h1>
    <h2>Lecture 5: More advanced linear models</h2>
    <p><br/></p>
  </hgroup>
  <article></article>  
</slide>
    

    <!-- SLIDES -->
    <slide class="" id="slide-1" style="background:;">
  <hgroup>
    <h2>Resources</h2>
  </hgroup>
  <article data-timings="">
    <p><img src="C:/Users/akane/Desktop/Science/Teaching/BL6024_UCC_2017/Lecture%2005/assets/img/crawley.jpg" alt="General Stats"></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Resources</h2>
  </hgroup>
  <article data-timings="">
    <p><img src="C:/Users/akane/Desktop/Science/Teaching/BL6024_UCC_2017/Lecture%2005/assets/img/zuur.jpg" alt="GLMs"></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>Defined</h3>

<p>In multiple regression we have a continuous response variable and two or more continuous
explanatory variables. </p>

<p>\[Y_i  = \beta_0 + \beta_1 X_1 + \beta_2 X_2 + ...+ \beta_k X_k\]</p>

<p>Some issues with multiple regression:</p>

<ul>
<li>the explanatory variables are often correlated with one another</li>
<li>there might be interactions between explanatory variables</li>
</ul>

<p>Remember parsimony - if a simpler model explains your data just as well as more complex model pick the simple one.</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>Example</h3>

<p>Abundances of around 75 invertebrate species from 45 sites were measured on various beaches along the Dutch coast. We want to see if a suite of environmental data can explain species richness. </p>

<p>We have 5 explanatory variables including the variable &quot;NAP&quot; which measured the height of the sample site compared with average sea level, and indicated the time a site is under water. Week is fitted as a nominal variable. </p>

<p>\[richness  = \beta_0 + \beta_1 * NAP + \beta_2*grainsize + \beta_3*hummus + week + \\\beta_4 * angle + \epsilon\]</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>What do the data look like?</h3>

<p>Take a look at correlations between variables</p>

<pre><code class="r">plot(rich)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>Fit a model using lm</h3>

<pre><code class="r">model1&lt;-lm(Richness ~ NAP+grainsize+humus+factor(week) + angle2, 
                 data = rich)
summary(model1)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>Interpreting the regression parameters</h3>

<p>\(\beta_1\) shows the change in species richness for a one-unit change in NAP, while keeping all other variables constant. </p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>Model selection</h3>

<p>One aim of regression modelling is to find the optimal model that identifies the parameters that best explain the collected data.</p>

<p>Akaike Information Criteria (AIC) and adjusted \(R^2\) are often used to select between models. </p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>Model selection - AIC</h3>

<p>\[AIC= nlog(SS_{residual})+2(p+1)-nlog(n)\]</p>

<p>The first part of the AIC definition is a measure of goodness of fit. The second
part is a penalty for the number of parameters in the model. The lower the AIC the better. AIC values need to be compared to other models, they&#39;re not really interpretable on their own. A rule of thumb is that a difference in AIC &gt; 2 is an improvement.</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>Model selection - adjusted R<sup>2</sup></h3>

<p>\[adjusted\,\, R^2=1-\frac{SS_{residual}/(n-(p+1))}{SS_{total}/n-1}\]</p>

<p>where p is the number of parameters in the model and n is the sample size</p>

<p>The disadvantage of \(R^2\) is
that the more explanatory variables are used, the higher the \(R^2\) The adjusted \(R^2\)
accounts for different degrees of freedom and, hence, the extra regression parameters. The higher the adjusted \(R^2\) the better.</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>Model selection</h3>

<p>Rather than selecting explanatory variables randomly there are automatic procedures in R. </p>

<ul>
<li>Forward selection - start from simplest and move to complex based on AIC</li>
<li>Backward selection - start from most complex and move to simple based on AIC</li>
<li>Combination - do both based on AIC</li>
</ul>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>Backward selection</h3>

<pre><code class="r">max.model &lt;- lm(Richness~NAP+grainsize+humus+factor(week) + angle2,data=rich)
step(max.model, direction = &quot;backward&quot;)
</code></pre>

<h3>Forward selection</h3>

<pre><code class="r">min.model = lm(Richness ~ 1, data=rich)
max.model &lt;- formula(lm(Richness~NAP+grainsize+humus+factor(week) + angle2,data=rich))
step(min.model, direction = &quot;forward&quot;,max.model)
</code></pre>

<h3>Both ways selection</h3>

<pre><code class="r">min.model = lm(Richness ~ 1, data=rich)
max.model &lt;- formula(lm(Richness~NAP+grainsize+humus+factor(week) + angle2,data=rich))
step(min.model, direction = &quot;both&quot;,max.model)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>Inerpret and plot our final model</h3>

<pre><code class="r">intercepts &lt;- c(coef(model1)[&quot;(Intercept)&quot;],
                coef(model1)[&quot;(Intercept)&quot;] + coef(model1)[&quot;factor(week)2&quot;],
                coef(model1)[&quot;(Intercept)&quot;] + coef(model1)[&quot;factor(week)3&quot;],
                coef(model1)[&quot;(Intercept)&quot;] + coef(model1)[&quot;factor(week)4&quot;])

lines.df &lt;- data.frame(intercepts = intercepts,
                       slopes = rep(coef(model1)[&quot;NAP&quot;], 4),
                       week = levels(factor(rich$week)))

qplot(x = NAP, y = Richness, color = factor(week), data = rich) + 
  geom_abline(aes(intercept = intercepts, 
                  slope = slopes, 
                  color = week), data = lines.df)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>Inerpret and plot our final model</h3>

<p><img src="assets/fig/unnamed-chunk-10-1.png" title="plot of chunk unnamed-chunk-10" alt="plot of chunk unnamed-chunk-10" style="display: block; margin: auto;" /></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>Model selection - warning about algorithms</h3>

<ul>
<li>It yields R-squared values that are badly biased to be high.</li>
<li>It has severe problems in the presence of collinearity.</li>
<li>Increasing the sample size does not help very much</li>
<li>It allows us to not think about the problem, i.e. encourages data dredging </li>
</ul>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>Model selection - warning about algorithms</h3>

<pre><code class="r">set.seed(1)
N &lt;- 200000
y &lt;- rnorm(N)
x1 &lt;- y + rnorm(N)
x2 &lt;- y + rnorm(N)
x3 &lt;- y + rnorm(N)
x4 &lt;- rnorm(N)
x5 &lt;- rnorm(N)
x6 &lt;- x1 + x2 + x3 + rnorm(N)
data &lt;- data.frame(y, x1, x2, x3, x4, x5, x6)
fit1 &lt;- lm(y ~ ., data)
fit2 &lt;- lm(y ~ 1, data)
back&lt;-step(fit1,direction=&quot;backward&quot;)
forward&lt;-step(fit2,direction=&quot;forward&quot;,scope=list(upper=fit1,lower=fit2))
both&lt;-step(fit2,direction=&quot;both&quot;,scope=list(upper=fit1,lower=fit2))
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>Multicolinearity</h3>

<p>If some of the explanatory variables in a multiple regression analysis are highly correlated with one another, this is called collinearity or multicollinearity. This means that choosing the best subset of explanatory variables may be difficult</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>Multicolinearity - Variance inflation factors</h3>

<p>Collinearity can be detected by calculating the variance inflation factor (VIF)
for each explanatory variable</p>

<p>\[VIF_i=\frac{1}{1-R^2_i}\]</p>

<p>where \(R^2_i\) is the coefficient of determination you get from regressing the
each of the explanatory variable against all the other explanatory variables. </p>

<p>If it is uncorrelated with all the others then VIF = 1. VIF increases as the correlation
increases. Worry if \(VIF > 5\)</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>Multicolinearity - Variance inflation factors</h3>

<pre><code class="r">library(car)
vif(fit1)
</code></pre>

<pre><code>##        x1        x2        x3        x4        x5        x6 
##  3.514338  3.501349  3.501065  1.000042  1.000038 13.023041
</code></pre>

<pre><code class="r">vifTest&lt;-lm(x6~x1+x2+x3+x4+x5,data=data)
summary(vifTest)$r.squared
</code></pre>

<pre><code>## [1] 0.923213
</code></pre>

<pre><code class="r">1/(1-0.923213)
</code></pre>

<pre><code>## [1] 13.02304
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>Multicolinearity</h3>

<p>One potential solution to multicolinearity is to combine the colinear explanatory variables using a principal components analysis. Alternatively if two variables are extremely correlated, you can drop one of them because they are telling you the same thing. </p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>Interactions - it depends</h3>

<p>Interaction effects occur when the effect of one variable depends on the value of another variable. </p>

<p>If someone asks you, &quot;Do you prefer ketchup or chocolate sauce on your food?&quot; You&#39;ll say it depends on the type of food! That &#39;it depends&#39; is the nature of an interaction effect. </p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>Interactions - it depends</h3>

<pre><code class="r"># Load the Interactions_Categorical data
food&lt;-read.csv(&quot;C:\\Users\\akane\\Desktop\\Science\\Teaching\\BL6024_UCC_2017\\Lecture 05\\assets\\img\\Interactions_Categorical.csv&quot;)
intCat&lt;-lm(Enjoyment~Food*Condiment,data=food)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression - interactions</h2>
  </hgroup>
  <article data-timings="">
    <h3>Interactions - it depends</h3>

<pre><code class="r">interaction.plot(x.factor = food$Food, 
                  trace.factor = food$Condiment,
                  response = food$Enjoyment,ylab=&quot;enjoyment&quot;)
</code></pre>

<p><img src="assets/fig/unnamed-chunk-15-1.png" title="plot of chunk unnamed-chunk-15" alt="plot of chunk unnamed-chunk-15" style="display: block; margin: auto;" /></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression - interactions</h2>
  </hgroup>
  <article data-timings="">
    <h3>Dangerous to overlook interaction effects</h3>

<p>Based on these plots, we would choose hot dogs with chocolate sauce because they each produce higher enjoyment. </p>

<p><img src="assets/fig/unnamed-chunk-16-1.png" title="plot of chunk unnamed-chunk-16" alt="plot of chunk unnamed-chunk-16" style="display: block; margin: auto;" /></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>What do interactions of continuous variables look like?</h3>

<pre><code class="r">set.seed(1)
x1 &lt;- runif(100, 0, 1)
x2 &lt;- sample(1:10, 100, TRUE)/10
y &lt;- 1 + 2 * x1 + 3 * x2 - 10 * x1 * x2 + rnorm(100)
m.int &lt;- lm(y ~ x1 * x2)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression</h2>
  </hgroup>
  <article data-timings="">
    <h3>What do interactions of continuous variables look like?</h3>

<pre><code class="r">library(rsm)
persp(m.int, x1 ~ x2, zlab = &quot;y&quot;,col = rainbow(50), contours = &quot;colors&quot;,
      theta = 120, phi = 15,ticktype=&quot;simple&quot;)
</code></pre>

<p><img src="assets/fig/unnamed-chunk-18-1.png" title="plot of chunk unnamed-chunk-18" alt="plot of chunk unnamed-chunk-18" style="display: block; margin: auto;" /></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression - interactions</h2>
  </hgroup>
  <article data-timings="">
    <h3>Without the interaction</h3>

<pre><code class="r">set.seed(1)
x1 &lt;- runif(100, 0, 1)
x2 &lt;- sample(1:10, 100, TRUE)/10
y &lt;- 1 + 2 * x1 + 3 * x2 + rnorm(100)
m.int2 &lt;- lm(y ~ x1 + x2)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Multiple Regression - interactions</h2>
  </hgroup>
  <article data-timings="">
    <h3>Without the interaction</h3>

<pre><code class="r">library(rsm)
persp(m.int2, x1 ~ x2, zlab = &quot;y&quot;,col = rainbow(50), contours = &quot;colors&quot;,
      theta = 35, phi = 15,ticktype=&quot;simple&quot;)
</code></pre>

<p><img src="assets/fig/unnamed-chunk-20-1.png" title="plot of chunk unnamed-chunk-20" alt="plot of chunk unnamed-chunk-20" style="display: block; margin: auto;" /></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Normality</h3>

<p>In ecological research we typically only get one value of y per x e.g. one measurement of height per age. We then assume the data point we have comes from a normal distribution. </p>

<p>When we fit the regression line it is as if it is running through a load of normal distributions, one for each x data point. Because the line won&#39;t fit through the points perfectly we&#39;re left with some residual variance around it. That&#39;s what we mean when we say the residuals are normally distributed. </p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Normality</h3>

<p><img src="assets/fig/unnamed-chunk-21-1.png" alt="plot of chunk unnamed-chunk-21"></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>LM to GLM</h3>

<p>Linear models assume that your residuals:</p>

<ul>
<li>are normally distributed and </li>
<li>have constant variance. </li>
</ul>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Normally distributed data with constant variance</h3>

<p><img src="assets/fig/unnamed-chunk-22-1.png" title="plot of chunk unnamed-chunk-22" alt="plot of chunk unnamed-chunk-22" style="display: block; margin: auto;" /></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Artificially messy data</h3>

<p><img src="assets/fig/unnamed-chunk-23-1.png" title="plot of chunk unnamed-chunk-23" alt="plot of chunk unnamed-chunk-23" style="display: block; margin: auto;" /></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Real world data</h3>

<p><img src="assets/fig/unnamed-chunk-24-1.png" title="plot of chunk unnamed-chunk-24" alt="plot of chunk unnamed-chunk-24" style="display: block; margin: auto;" /></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>When simple LMs aren&#39;t up to the job</h3>

<p>Generalized linear models enable you to fit models to data that don&#39;t meet the requirements of normality and equal variance in your residuals. </p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Types of data GLMs can handle</h3>

<p>Specifically, you should use GLMs when the response variable is:</p>

<ul>
<li>count data expressed as proportions (e.g. logistic regressions);</li>
<li>count data that are not proportions (e.g. log-linear models of counts);</li>
<li>binary response variables (e.g. dead or alive);</li>
<li>data on time to death where the variance increases faster than linearly with the mean (e.g. time data with gamma errors).</li>
</ul>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Properties</h3>

<p>A generalized linear model has three important properties:</p>

<ul>
<li>the error structure;</li>
<li>the linear predictor;</li>
<li>the link function.</li>
</ul>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>The error structure - random component</h3>

<p>You may specify a variety of different error distributions with a GLM:</p>

<ul>
<li>Poisson errors, useful with count data;</li>
<li>binomial errors, useful with data on proportions;</li>
<li>gamma errors, useful with data showing a constant coefficient of variation;</li>
<li>exponential errors, useful with data on time to death (survival analysis).</li>
</ul>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>The linear predictor - systematic component</h3>

<p>This specifies the explanatory variables (\(X_1, X_2, ... X_k\)) in the model, more specifically their linear combination in creating the so called linear predictor; e.g., the linear prediction function is equal to \(\beta_0 + \beta_1x_1 + \beta_2x_2 + \beta_kx_k\) as we have seen in linear regression.</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>The link function</h3>

<p>The link function specifies the link between the random (error structure) and systematic components (linear predictor). It says how the expected value (the mean) of the response (your y value) relates to the linear predictor of explanatory variables</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>The link function</h3>

<p>An important criterion in
the choice of link function is to ensure that the fitted values stay within reasonable
bounds. We would want to ensure, for example, that counts were all greater than or
equal to zero (negative count data would be nonsense). </p>

<p>Similarly, if the response variable was the proportion of individuals who died, then the fitted values would have to lie between 0 and 1 (fitted values greater than 1 or less than 0 would be meaningless).</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Probability distribution centric convention</h3>

<p>Strange to think of the stochastic parts of the model as errors. E.g., say you are looking at the relation between height and weight by running a linear regression with height as the predictor and weight as the outcome, where is the error in this model? </p>

<p>Using the error term convention the difference between the resulting regression line and the weights would be labeled as the error. But isn’t it strange to talk about the fact that there is variability in how much people weigh, given their height, as an error? </p>

<p>Using the distribution centric notation this difference is seen more like a part of the actual model rather than a nuisance measurement error.</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Probability distribution centric convention</h3>

<p>Instead of this:
\[y_i=\beta_0+\beta_1 x_i + \epsilon_i\]
\[\epsilon_i \sim Normal (0,\sigma^2)\]
we could write the regression equation as this:
\[y_i\sim Normal (\mu_i,\sigma^2)\]
\[\mu_i=\beta_0+\beta_1 x_i\]</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Ice-cream example</h3>

<p>Does the number of icecreams sold vary with temperature?</p>

<pre><code class="r"># load in the icecream data file
icecream&lt;-read.csv(&quot;C:\\Users\\akane\\Desktop\\Science\\Teaching\\BL6024_UCC_2017\\Lecture 05\\assets\\img\\icecream.csv&quot;)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Plot it</h3>

<p><img src="assets/fig/unnamed-chunk-27-1.png" title="plot of chunk unnamed-chunk-27" alt="plot of chunk unnamed-chunk-27" style="display: block; margin: auto;" /></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Fit a GLM with family=gaussian</h3>

<p>We&#39;re arguing that the observation $y_i $was drawn from a Normal (aka Gaussian) distribution with a mean \(\mu_i\), depending on the temperature \(x_i\) and a constant variance \(\sigma^2\) across all temperatures. </p>

<p>If we think in terms of distributions our model looks like this:
\[y_i\sim N(\mu,\sigma^2)\]
\[E[y_i]=\mu_i=\beta_0+\beta_1 x_i\]</p>

<p>We use the <em>GLM</em> command in R and we set the family to gaussian because we&#39;re assuming normally distributed errors. The link function linking the error structure to the linear predictor is called the identity function because we don&#39;t need to transform the relationship, it&#39;s already on a linear scale. </p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Model interpretation</h3>

<pre><code class="r">model1 &lt;- glm(units ~ temp, data=icecream, 
              family=gaussian(link=&quot;identity&quot;))
summary(model1)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Model interpretation</h3>

<p>What this tells us is that: 
\(y_i\sim N(\mu_i,\sigma^2)\) where \(\mu_i = -159.5 + 30.1x_i\)</p>

<p>Therefore, when it&#39;s 20 degrees out we&#39;d expect to sell \(-159.5 + 30.1*20 = 442.5\) units of ice-cream. </p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLM with normal errors cont.</h2>
  </hgroup>
  <article data-timings="">
    <h3>what are these deviance measures?</h3>

<p>Deviance is a measure of goodness of fit of a model. Higher numbers always indicate bad fit. It is just the sum of square errors when you use family = gaussian().</p>

<p>The null deviance shows how well the response variable is predicted by a model that includes only the intercept (grand mean) whereas the residual deviance takes into account the independent variables.</p>

<p>You can see that the addition of 1 (11-10 = 1) independent variable decreased the deviance from 174755 to 14536. The Residual Deviance has reduced by (174755 - 14536 = 160219) with a loss of one degree of freedom.</p>

<p>If your Null Deviance is really small, it means that the Null Model explains the data pretty well. Likewise, with your Residual Deviance.</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLM with normal errors cont.</h2>
  </hgroup>
  <article data-timings="">
    <h3>what are these deviance measures?</h3>

<pre><code class="r">interceptOnly &lt;- glm(units ~ 1, data=icecream, 
              family=gaussian(link=&quot;identity&quot;))
summary(interceptOnly)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Problems with our model choice</h3>

<p>When it&#39;s 20 degrees out we&#39;d expect to sell \(-159.5 + 30.1*0 = -159.5\) units of icecream. </p>

<p>That doesn&#39;t make much sense. The intercept is at -159, which would mean that customers return on average 159 ice creams on a freezing day. Perhaps a different conditional distribution for our y data would work better here because we&#39;re dealing with count data. </p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Count data</h3>

<p>Count data are whole numbers (integers) so that when the mean is low, the data are likely to consist only of zeros, ones and twos, with the odd three of four thrown in. This being the case, the variance of count data is bound to be low when the mean is low.</p>

<p>However, when the mean of count data is high, the range of individual counts can be from
zero to potentially very large numbers, so we can expect to obtain a  high variance. </p>

<p>For count data, therefore, the variance is expected to increase with the mean, rather than being constant as assumed in previous linear models.</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Count data</h3>

<p>Straightforward linear regression methods (assuming constant variance and normal
errors) are not appropriate for count data for four main reasons:</p>

<ul>
<li>the linear model might lead to the prediction of negative counts</li>
<li>the variance of the response variable is likely to increase with the mean</li>
<li>the errors will not be normally distributed</li>
<li>zeros are difficult to handle in transformations</li>
</ul>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Count data - link function</h3>

<p>The Poisson distribution has only one parameter, here \(\mu_i\), which is its expected value or mean. The link function for \(\mu_i\) is the logarithm, i.e. the logarithm of the expected value is regarded as a linear function of the predictors. </p>

<p>The advantage of the log link is that the fitted values are always positive, regardless of the values of the covariates and the estimated regression parameters.</p>

<p>But this means you have to use exponentiation (the reverse of logs) to get back to the original scale.</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Count data</h3>

<p>The model looks like this:
\[y_i\sim Poisson(\mu_i)\]
\[E[y_i]=\mu_i=exp(\beta_0+\beta_1 x_i)\]
\[log(\mu_i)=\beta_0+\beta_1 x_i\]</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Create our model for count data</h3>

<p>In R we define this model using family = poisson and supplying the link function which is the log. </p>

<pre><code class="r">model2 &lt;- glm(units ~ temp, data=icecream, 
              family=poisson(link=&quot;log&quot;))
</code></pre>

<p>We can also simply specify the family and by default R will use the log link function. </p>

<pre><code class="r">model2 &lt;- glm(units ~ temp, data=icecream, 
              family=poisson)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Interpreting our results</h3>

<p>Remember we have to exponentiate these parameter values to get them back on a familiar scale because of the following:</p>

<p>\[y_i\sim Poisson(\mu_i)\]
\[E[y_i]=\mu_i=exp(\beta_0+\beta_1 x_i)\]
\[log(\mu_i)=\beta_0+\beta_1 x_i\]</p>

<p>If we do so, how many ice creams do we predict will be sold at \(0^0C\)?</p>

<pre><code class="r">exp(4.543821 + 0.075595 * 0 )
</code></pre>

<pre><code>## [1] 94.04948
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <p>Interpreting the slope alone. </p>

<pre><code class="r">exp(0.075595) - 1
</code></pre>

<pre><code>## [1] 0.07852568
</code></pre>

<p>For a one unit change in temperature there is an approximate 8% increase in icecreams sold. </p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Another dataset - Fish abundance at different depths</h3>

<pre><code class="r"># load in the Fish data file 
Fish &lt;- read.table(&quot;C:\\Users\\akane\\Desktop\\Science\\Teaching\\BL6024_UCC_2017\\Lecture 05\\assets\\img\\Fish.txt&quot;,header = T)
head(Fish,3)
</code></pre>

<pre><code>##   Site TotAbund        Dens MeanDepth Year Period       Xkm       Ykm
## 1    1       76 0.002070281     0.804 1978      1  98.75575 -57.46692
## 2    2      161 0.003519799     0.808 2001      2  76.80388 178.64798
## 3    3       39 0.000980515     0.809 2001      2 103.79283 -50.05184
##   SweptArea
## 1  36710.00
## 2  45741.25
## 3  39775.00
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Plot our data</h3>

<p><img src="assets/fig/unnamed-chunk-35-1.png" title="plot of chunk unnamed-chunk-35" alt="plot of chunk unnamed-chunk-35" style="display: block; margin: auto;" /></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>What&#39;s an appropriate model? One with Normal errors?</h3>

<p>\[Abundance_i \sim \beta_1 + \beta_2 Mean\,Depth_i +\epsilon_i\]
\[\epsilon_i\sim N(0,\sigma^2)\]</p>

<pre><code class="r">m1 &lt;- glm(TotAbund~MeanDepth,data=Fish)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>What do our diagnostic plots look like?</h3>

<p><img src="assets/fig/unnamed-chunk-37-1.png" alt="plot of chunk unnamed-chunk-37"></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Model fit with simulated normal curves of total abundance superimposed</h3>

<p><img src="assets/fig/unnamed-chunk-38-1.png" title="plot of chunk unnamed-chunk-38" alt="plot of chunk unnamed-chunk-38" style="display: block; margin: auto;" /></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Let&#39;s fit a GLM with family = Poisson</h3>

<pre><code class="r">m1 &lt;- glm(TotAbund~ MeanDepth, data= Fish, family = poisson)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Interpret the model</h3>

<p>Draw out the regression equation and input the parameter values from the model output:</p>

<p>\[log(\mu_i)=6.643-0.628*Mean\,\,depth\]</p>

<p>\[\mu_i=e^{log(\mu_i)}=e^{6.643-0.628*Mean\,depth}\]</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Plot the fitted line</h3>

<pre><code class="r">par(mar = c(5,5,2,2))
# pick the min and max depths and create a vector of length 25 with equally spaced values
MyData &lt;- data.frame(MeanDepth = seq(0.804, 4.865, length = 25))
# predict function fills in the predicted y data along the curve 
P1 &lt;- predict(m1, newdata = MyData, type = &quot;response&quot;)
plot(x = Fish$MeanDepth,
     y = Fish$TotAbund,
     ylim = c(0,1300),
     xlab = &quot;Mean depth (km)&quot;,
     ylab = &quot;Total abundance values&quot;, cex.lab = 1.5)

lines(MyData$MeanDepth, P1, lwd = 3)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Plot the fitted line</h3>

<p><img src="assets/fig/unnamed-chunk-41-1.png" title="plot of chunk unnamed-chunk-41" alt="plot of chunk unnamed-chunk-41" style="display: block; margin: auto;" /></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Plot simulated values from Poisson distribution with mean given by line</h3>

<p><img src="assets/fig/unnamed-chunk-42-1.png" title="plot of chunk unnamed-chunk-42" alt="plot of chunk unnamed-chunk-42" style="display: block; margin: auto;" /></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Pseudo R<sup>2</sup></h3>

<p>We don&#39;t have a \(R^2\) in GLM models; the closest we can get is the explained deviance, which is calulated as: </p>

<p>\[100 *  \frac{null\,\,\,\,deviance-residual\,\,\,deviance}{null\,\,\,\,deviance}\]</p>

<pre><code class="r">100* ((m1$null.deviance-m1$deviance) / m1$null.deviance)
</code></pre>

<pre><code>## [1] 43.23067
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Model validation</h3>

<p>Let&#39;s look at Pearson residuals which work well for GLMs. They&#39;re just standardised residuals. Whereas regular residuals are defined as the observed values minus the fitted values:
\[\epsilon_i=TotAbund_i-\mu_i\]
The Pearson residuals are calculated by:
\[\epsilon_i=\frac{TotAbund_i-\mu_i}{\sqrt{variance(TotalAbund_i)}}\]
Note, that we&#39;re not looking for normality in the Pearson residuals, the issue is lack of fit. The phrase &#39;Poisson errors&#39; should not be used as errors are not Poisson distributed. </p>

<pre><code class="r">E1&lt;-resid(m1,type=&quot;pearson&quot;)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Model validation</h3>

<pre><code class="r">E1&lt;-resid(m1,type=&quot;pearson&quot;)
F1&lt;-fitted(m1)
eta&lt;-predict(m1,type=&quot;link&quot;)
plot(x = eta, 
     y = E1,
     xlab = &quot;Fitted values&quot;,
     ylab = &quot;Pearson residuals&quot;,
     cex.lab = 1.5)
abline(h=0, v = 0, lty = 2)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Clear increase in variation for larger fitted values</h3>

<p><img src="assets/fig/unnamed-chunk-46-1.png" alt="plot of chunk unnamed-chunk-46"></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Overdispersion</h3>

<p>This is extra, unexplained variation in the response than would be expected based on your statistical model of choice, here the Poisson model. Reasons include:</p>

<ul>
<li>Model is missing a required explanatory variable </li>
<li>Model has outliers</li>
<li>Model requires interaction terms</li>
<li>Explanatory variables are not on the right scale, e.g. log scale</li>
<li>A continuous covariate has a non-linear effect</li>
<li>Model has the wrong link function</li>
<li>There is zero-inflation</li>
<li>There is some dependency structure in the data
In many cases overdispersion affects model coefficients. We can test for overdispersion by seeing if the residual deviance is much greater than the residual degrees of freedom. Their ratio should be ~ 1. </li>
</ul>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Overdispersion - causes?</h3>

<pre><code class="r">m1 &lt;- glm(TotAbund~ MeanDepth, data= Fish, family = poisson)
deviance(m1) / df.residual(m1)
</code></pre>

<pre><code>## [1] 109.5122
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Overdispersion - causes?</h3>

<pre><code>##   Site TotAbund        Dens MeanDepth Year Period       Xkm       Ykm
## 1    1       76 0.002070281     0.804 1978      1  98.75575 -57.46692
## 2    2      161 0.003519799     0.808 2001      2  76.80388 178.64798
## 3    3       39 0.000980515     0.809 2001      2 103.79283 -50.05184
## 4    4      410 0.008039216     0.848 1979      1  91.53227 146.44797
## 5    5      177 0.005933375     0.853 2002      2 107.14419 -37.07544
## 6    6      695 0.021800502     0.960 1980      1  86.56470 -48.19807
##   SweptArea
## 1  36710.00
## 2  45741.25
## 3  39775.00
## 4  51000.00
## 5  29831.25
## 6  31880.00
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Overdispersion - causes?</h3>

<p>The data were collected during two different periods (1979-1989 &amp; 1997-2002) so that may be important to consider. </p>

<p>There may be an interaction between depth and time period so we should test to see if that affects our overdispersion measure as well. </p>

<p>\[TotAbund_i \sim Poisson(\mu_i)\]
\[log(\mu_i)=\beta_1+\beta_2 Mean\,\,depth + \beta_3 Period_i + \beta_4 Mean\,\,depth_i *Period\]</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Include time period as a factor and the interaction term</h3>

<pre><code class="r"># classify period as a factor variable 
Fish$fPeriod &lt;- factor(Fish$Period)
m2 &lt;- glm(TotAbund~ MeanDepth * fPeriod, data= Fish, family = poisson)
summary(m2)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Model interpretation</h3>

<p>We now have two lines with different intercepts and slopes. 
\[Period1:log(\mu_i)=6.832-0.658*Mean\,\,depth_i\]
\[Period2:log(\mu_i)=6.832-0.674+(-0.658+0.115)*Mean\,\,depth_i\]</p>

<p>Hasn&#39;t improved our overdispersion problem though!</p>

<pre><code class="r">deviance(m2) / df.residual(m2)
</code></pre>

<pre><code>## [1] 100.6549
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Offsets</h3>

<p>Counts can be considered as rates at which events occur within areas of different sizes or as time periods of differing duration. We should adjust for this in our model. We want to adjust the total number of fish per site (TotAbund) by the size of the site, (SweptArea). </p>

<p>Our standard predictor looks like this:</p>

<p>\[log(\mu_i)=\beta_1+\beta_2X_i\]</p>

<p>Adding an offset looks like this:</p>

<p>\[log(\mu_i/t)=\beta_1+\beta_2X_i\]</p>

<p>which is equivalent to this: </p>

<p>\[log(\mu_i)=\beta_1+\beta_2X_i+log(t)\]</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>Overdispersion</h2>
  </hgroup>
  <article data-timings="">
    <h3>Offsets</h3>

<p>For our example:</p>

<p>\[TotAbund_i \sim Poisson(\mu_i)\]
\[log(\mu_i)=\beta_1+\beta_2*Mean\,\,depth_i+\beta_3*Period\\+\beta_4*Mean\,\,depth_i*Period_i+log(SweptArea)\]</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Offsets</h3>

<pre><code class="r">Fish$LogSA &lt;- log(Fish$SweptArea)
m3 &lt;- glm(TotAbund~ MeanDepth * fPeriod +offset(LogSA), data= Fish, family = poisson)
summary(m3)
</code></pre>

<p>Did this fix our overdispersion?</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Overdispersion - negative binomial solution</h3>

<p>One way to adjust for Poisson overdispersion is to use the negative binomial GLM. We have a clear case where the variance is larger than the mean. A negative binomial GLM can help us here. The link function is still the log, just as in Poisson.  </p>

<p>\[TotAbund_i \sim NB(\mu_i,k)\]</p>

<p>\[log(\mu_i)=\beta_1+\beta_2*Mean\,\,depth_i+\beta_3*Period\\+\beta_4*Mean\,\,depth_i*Period_i+log(SweptArea)\]</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Overdispersion - negative binomial solution</h3>

<p>We fit the model using the function glm.nb, this function is located in the MASS package. Note we don&#39;t use family here. </p>

<pre><code class="r">require(MASS)
m4 &lt;- glm.nb(TotAbund~ MeanDepth * fPeriod + offset(LogSA), data= Fish)
summary(m4)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Overdispersion - negative binomial solution</h3>

<p>Has the negative binomial GLM fixed our overdispersion problem?</p>

<pre><code class="r">deviance(m4) / df.residual(m4)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Negative binomial - model selection</h3>

<p>The interaction from our summary of m4 looks to be non-significant at p = 0.05. We could comapare a model with the interaction to a model without the interaction. Our null is that the regression parameter \(\beta_4\) for the interaction term equals 0 and that the deviances are equal. The command drop1 does this in R. </p>

<pre><code class="r">drop1(m4,test=&quot;Chi&quot;)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Negative binomial - model selection</h3>

<p>The results show the p-value is &gt;&gt;&gt; 0.05 so we fail to reject the null that the regression parameter for the interaction term is different from 0. Therefore we can drop it. </p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Negative binomial - model selection</h3>

<p>Refit the model without the interaction term. </p>

<pre><code class="r">m5 &lt;- glm.nb(TotAbund~ MeanDepth + fPeriod + offset(LogSA), data= Fish)
summary(m5)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Negative binomial - model validation</h3>

<pre><code class="r"># Model validation
E1 &lt;- resid(m5, type = &quot;pearson&quot;)
F1 &lt;- fitted(m5)
eta &lt;- predict(m5, type = &quot;link&quot;)
par(mfrow = c(2, 2), mar = c(5, 5, 2, 2))
plot(x = F1, y = E1, xlab = &quot;Fitted values&quot;, ylab = &quot;Pearson residuals&quot;, cex.lab = 1.5)
abline(h = 0, v = 0, lty = 2)
plot(x = c(1:146), y = cooks.distance(m5), xlab = &quot;index&quot;, ylab = &quot;cooks distance&quot;, 
    ylim = c(0:1), cex.lab = 1.5)
plot(x = Fish$MeanDepth, y = E1, xlab = &quot;Mean Depth (km)&quot;, ylab = &quot;Pearson residuals&quot;, 
    cex.lab = 1.5, pch = 16)
abline(h = 0, v = 0, lty = 2)
boxplot(E1 ~ Period, ylab = &quot;Pearson residuals&quot;, data = Fish, cex.lab = 1.5, 
    xlab = &quot;Period&quot;)
abline(h = 0, v = 0, lty = 2)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Negative binomial - model validation</h3>

<p><img src="assets/fig/unnamed-chunk-58-1.png" title="plot of chunk unnamed-chunk-58" alt="plot of chunk unnamed-chunk-58" style="display: block; margin: auto;" /></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Negative binomial - model interpretation</h3>

<p>\[TotAbund_i \sim NB(\mu_i,1.94)\]</p>

<p>\[Period1: \mu_i=e^{\eta_i}; \eta_i=-3.31-1.01*Mean\,\,depth_i+log(SweptArea_i) \]</p>

<p>\[Period2: \mu_i=e^{\eta_i}; \eta_i=-3.31-0.43-1.01*Mean\,\,depth_i+log(SweptArea_i) \]</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Negative binomial - plot the final model</h3>

<p><img src="assets/fig/unnamed-chunk-59-1.png" title="plot of chunk unnamed-chunk-59" alt="plot of chunk unnamed-chunk-59" style="display: block; margin: auto;" /></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Binary data</h3>

<p>Is honeycomb cell size related to the presence of honeybee parasites? </p>

<pre><code class="r"># load in the WBees data file
Bees &lt;- read.table(&quot;C:\\Users\\akane\\Desktop\\Science\\Teaching\\BL6024_UCC_2017\\Lecture 05\\assets\\img\\WBees.txt&quot;,header=T)
head(Bees)
</code></pre>

<pre><code>##   Parasites CellSize
## 1         0    0.424
## 2         0    0.454
## 3         0    0.457
## 4         0    0.468
## 5         0    0.493
## 6         0    0.558
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Binary data - plot it</h3>

<p><img src="assets/fig/unnamed-chunk-61-1.png" title="plot of chunk unnamed-chunk-61" alt="plot of chunk unnamed-chunk-61" style="display: block; margin: auto;" /></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Binary data - distribution of our response variable</h3>

<p>We&#39;re assuming that our \(Y_i\) data has a Bernoulli distribution with a probability \(\pi_i\) which is a a binomial distribution with 1 independent trial. Remember \(\pi\) is the probability of success (y=1) for each trial. </p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Binary data - the link function</h3>

<p>The actual parameter (e.g. probability for a binomial response) cannot range from negative infinity to positive infinity, but your predicted parameter will. The identity link function, i.e. the one used for a normally distributed y, gives illogical results, probabilites and realisations that are &lt; 0 and &gt; 1.</p>

<p>We want a link function that transforms the values of our linear predictor \(\beta_1+\beta_2*Cell\,\,size_i\) from \((-\infty,\infty)\) to \((0,1)\), since logistic regression predicts probabilities of success, i.e. the mean of Bernoulli distribution.</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>Binary data - the link function</h3>

<p>We can use odds, defined as:\(\frac{probabilty_i}{1-probability_i}\) to get rid of the upper boundary issue e.g. probability of 0.9 = odds of 9.0. We take the log of the odds so we get rid of the lower boundary e.g. log odds of a probability of 0.1 = -2.20. The resulting link function is called the logit. </p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLMs</h2>
  </hgroup>
  <article data-timings="">
    <h3>log odds plotted against probabilites</h3>

<p><img src="assets/fig/unnamed-chunk-62-1.png" title="plot of chunk unnamed-chunk-62" alt="plot of chunk unnamed-chunk-62" style="display: block; margin: auto;" /></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLM</h2>
  </hgroup>
  <article data-timings="">
    <h3>odds and probabilities</h3>

<p>If the probability of an event is 0.2, then the odds of it occurring are:
\[odds=\frac{0.2}{1-0.2}=0.25\]
the log odds of it occurring are: 
\[log\,\,odds=log(\frac{0.2}{1-0.2})=−1.3863\]
then the probability of it can be reconstructed as:
\[\frac{exp(ln(odds))}{1+exp(ln(odds))}=\frac{exp(1.3863)}{1+exp(1.3863)}\\=\frac{0.25}{1.25}=0.2\]</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLM</h2>
  </hgroup>
  <article data-timings="">
    <h3>odds and probabilities</h3>

<p>So we can translate from odds to probability \(\pi_i\) as follows:
\[log\frac{\pi_i}{1-\pi_i}=\beta_1+\beta_2X_i \\\frac{\pi_i}{1-\pi_i}=e^{\beta_1+\beta_2X_i}\\
\pi_i=(1-\pi_i)e^{\beta_1+\beta_2X_i}\\
\pi_i=e^{\beta_1+\beta_2X_i}-e^{\beta_1+\beta_2X_i}\pi_i\\\pi_i+e^{\beta_1+\beta_2X_i}\pi_i=e^{\beta_1+\beta_2X_i}\\(1+e^{\beta_1+\beta_2X_i})\pi_i=e^{\beta_1+\beta_2X_i}\\\pi_i=\frac{e^{\beta_1+\beta_2X_i}}{1+e^{\beta_1+\beta_2X_i}}\]</p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLM</h2>
  </hgroup>
  <article data-timings="">
    <h3>fitting this in R is easy</h3>

<pre><code class="r">B1&lt;-glm(Parasites~CellSize, family = binomial,data=Bees)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLM</h2>
  </hgroup>
  <article data-timings="">
    <h3>Interpreting results</h3>

<p>Both the intercept and the slope are significant so it&#39;s a question of putting everything together and getting it on a scale we can understand. </p>

<p>\[\pi_i=\frac{e^{\eta_i}}{1+e^{\eta_i}} \\ where \,\,\eta_i=-11.245+22.175 * Cell\,\,size_i\]</p>

<pre><code class="r">-11.245+22.175*0.5# odds scale
</code></pre>

<pre><code>## [1] -0.1575
</code></pre>

<pre><code class="r">exp(-11.245+22.175*0.5) / (1 + exp (-11.245+22.175*0.5))# probability scale
</code></pre>

<pre><code>## [1] 0.4607062
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLM</h2>
  </hgroup>
  <article data-timings="">
    <h3>Plot the fitted values</h3>

<pre><code class="r">mydata&lt;-data.frame(CellSize=seq(0.35,0.69,length=50))
pred &lt;- predict(B1,newdata=mydata,type=&quot;response&quot;)
plot(x=Bees$CellSize,y=Bees$Parasites,xlab=&quot;cell size&quot;, ylab=&quot;probability of parasites&quot;)
lines(mydata$CellSize,pred)
</code></pre>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLM</h2>
  </hgroup>
  <article data-timings="">
    <h3>Plot the fitted values</h3>

<p><img src="assets/fig/unnamed-chunk-66-1.png" alt="plot of chunk unnamed-chunk-66"></p>

  </article>
</slide>

<slide class="class" id="id" style="background:;">
  <hgroup>
    <h2>GLM</h2>
  </hgroup>
  <article data-timings="">
    <h3>Plot the fitted values</h3>

<p>Model validation for this sort of data is difficult because we&#39;re working with 1s and 0s i.e. binary data. </p>

  </article>
</slide>

    <slide class="backdrop"></slide>
  </slides>
  <div class="pagination pagination-small" id='io2012-ptoc' style="display:none;">
    <ul>
      <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=1 title='Resources'>
         1
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=2 title='Resources'>
         2
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=3 title='Multiple Regression'>
         3
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=4 title='Multiple Regression'>
         4
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=5 title='Multiple Regression'>
         5
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=6 title='Multiple Regression'>
         6
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=7 title='Multiple Regression'>
         7
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=8 title='Multiple Regression'>
         8
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=9 title='Multiple Regression'>
         9
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=10 title='Multiple Regression'>
         10
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=11 title='Multiple Regression'>
         11
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=12 title='Multiple Regression'>
         12
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=13 title='Multiple Regression'>
         13
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=14 title='Multiple Regression'>
         14
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=15 title='Multiple Regression'>
         15
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=16 title='Multiple Regression'>
         16
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=17 title='Multiple Regression'>
         17
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=18 title='Multiple Regression'>
         18
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=19 title='Multiple Regression'>
         19
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=20 title='Multiple Regression'>
         20
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=21 title='Multiple Regression'>
         21
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=22 title='Multiple Regression'>
         22
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=23 title='Multiple Regression - interactions'>
         23
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=24 title='Multiple Regression - interactions'>
         24
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=25 title='Multiple Regression'>
         25
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=26 title='Multiple Regression'>
         26
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=27 title='Multiple Regression - interactions'>
         27
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=28 title='Multiple Regression - interactions'>
         28
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=29 title='GLMs'>
         29
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=30 title='GLMs'>
         30
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=31 title='GLMs'>
         31
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=32 title='GLMs'>
         32
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=33 title='GLMs'>
         33
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=34 title='GLMs'>
         34
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=35 title='GLMs'>
         35
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=36 title='GLMs'>
         36
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=37 title='GLMs'>
         37
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=38 title='GLMs'>
         38
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=39 title='GLMs'>
         39
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=40 title='GLMs'>
         40
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=41 title='GLMs'>
         41
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=42 title='GLMs'>
         42
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=43 title='GLMs'>
         43
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=44 title='GLMs'>
         44
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=45 title='GLMs'>
         45
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=46 title='GLMs'>
         46
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=47 title='GLMs'>
         47
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=48 title='GLMs'>
         48
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=49 title='GLM with normal errors cont.'>
         49
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=50 title='GLM with normal errors cont.'>
         50
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=51 title='GLMs'>
         51
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=52 title='GLMs'>
         52
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=53 title='GLMs'>
         53
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=54 title='GLMs'>
         54
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=55 title='GLMs'>
         55
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=56 title='GLMs'>
         56
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=57 title='GLMs'>
         57
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=58 title='GLMs'>
         58
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=59 title='GLMs'>
         59
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=60 title='GLMs'>
         60
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=61 title='GLMs'>
         61
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=62 title='GLMs'>
         62
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=63 title='GLMs'>
         63
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=64 title='GLMs'>
         64
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=65 title='GLMs'>
         65
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=66 title='GLMs'>
         66
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=67 title='GLMs'>
         67
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=68 title='GLMs'>
         68
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=69 title='GLMs'>
         69
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=70 title='GLMs'>
         70
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=71 title='GLMs'>
         71
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=72 title='GLMs'>
         72
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=73 title='GLMs'>
         73
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=74 title='GLMs'>
         74
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=75 title='GLMs'>
         75
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=76 title='GLMs'>
         76
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=77 title='GLMs'>
         77
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=78 title='GLMs'>
         78
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=79 title='GLMs'>
         79
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=80 title='Overdispersion'>
         80
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=81 title='GLMs'>
         81
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=82 title='GLMs'>
         82
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=83 title='GLMs'>
         83
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=84 title='GLMs'>
         84
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=85 title='GLMs'>
         85
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=86 title='GLMs'>
         86
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=87 title='GLMs'>
         87
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=88 title='GLMs'>
         88
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=89 title='GLMs'>
         89
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=90 title='GLMs'>
         90
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=91 title='GLMs'>
         91
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=92 title='GLMs'>
         92
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=93 title='GLMs'>
         93
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=94 title='GLMs'>
         94
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=95 title='GLMs'>
         95
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=96 title='GLMs'>
         96
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=97 title='GLMs'>
         97
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=98 title='GLM'>
         98
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=99 title='GLM'>
         99
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=100 title='GLM'>
         100
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=101 title='GLM'>
         101
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=102 title='GLM'>
         102
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=103 title='GLM'>
         103
      </a>
    </li>
    <li>
      <a href="#" target="_self" rel='tooltip' 
        data-slide=104 title='GLM'>
         104
      </a>
    </li>
  </ul>
  </div>  <!--[if IE]>
    <script 
      src="http://ajax.googleapis.com/ajax/libs/chrome-frame/1/CFInstall.min.js">  
    </script>
    <script>CFInstall.check({mode: 'overlay'});</script>
  <![endif]-->
</body>
  <!-- Load Javascripts for Widgets -->
  
  <!-- MathJax: Fall back to local if CDN offline but local image fonts are not supported (saves >100MB) -->
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [['$','$'], ['\\(','\\)']],
        processEscapes: true
      }
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <!-- <script src="https://c328740.ssl.cf1.rackcdn.com/mathjax/2.0-latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
  </script> -->
  <script>window.MathJax || document.write('<script type="text/x-mathjax-config">MathJax.Hub.Config({"HTML-CSS":{imageFont:null}});<\/script><script src="libraries/widgets/mathjax/MathJax.js?config=TeX-AMS-MML_HTMLorMML"><\/script>')
</script>
<!-- LOAD HIGHLIGHTER JS FILES -->
  <script src="libraries/highlighters/highlight.js/highlight.pack.js"></script>
  <script>hljs.initHighlightingOnLoad();</script>
  <!-- DONE LOADING HIGHLIGHTER JS FILES -->
   
  </html>